import os
import re
import requests
from io import BytesIO
from PIL import Image
from typing import TypedDict, Optional
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.schema import HumanMessage
from langgraph.graph import StateGraph, END
import concurrent.futures
from functools import partial
from tavily import TavilyClient
from urllib.parse import quote_plus # --- NEW IMPORT ---
import logging

# =======================================================================================
# TOOL 1: THE COMPARISON & EVALUATION WORKFLOW
# =======================================================================================

def choose_groq_model(prompt: str):
Â  Â  p = prompt.lower()
Â  Â  if any(x in p for x in ["python", "code", "algorithm", "bug", "function", "script"]):
Â  Â  Â  Â  return "openai/gpt-oss-20b"
Â  Â  else:
Â  Â  Â  Â  return "llama-3.1-8b-instant"

def query_groq(prompt: str, groq_api_key: str):
Â  Â  model = choose_groq_model(prompt)
Â  Â  headers = {"Authorization": f"Bearer {groq_api_key}", "Content-Type": "application/json"}
Â  Â  data = {"model": model, "messages": [{"role": "user", "content": prompt}], "max_tokens": 2048}
Â  Â  try:
Â  Â  Â  Â  resp = requests.post("https://api.groq.com/openai/v1/chat/completions", json=data, headers=headers)
Â  Â  Â  Â  if resp.status_code == 200:
Â  Â  Â  Â  Â  Â  content = resp.json()["choices"][0]["message"]["content"]
Â  Â  Â  Â  Â  Â  return f"**Model:** {model}\n\n{content}"
Â  Â  Â  Â  return f"âŒ Groq API Error: {resp.text}"
Â  Â  except Exception as e:
Â  Â  Â  Â  return f"âš ï¸ Groq Error: {e}"

def comparison_and_evaluation_tool(query: str, google_api_key: str, groq_api_key: str) -> str:
Â  Â  """
Â  Â  Runs a query through Gemini and Groq, has an AI judge evaluate the best response,
Â  Â  and formats everything into a comprehensive answer.
Â  Â  """
Â  Â  print("---TOOL: Executing the Comparison & Evaluation Workflow---")
Â  Â Â 
Â  Â  # Use the fast model for the head-to-head comparison
Â  Â  fast_llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", google_api_key=google_api_key)
Â  Â  # Use the powerful model for the critical task of judging
Â  Â  judge_llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", google_api_key=google_api_key)

Â  Â  with concurrent.futures.ThreadPoolExecutor() as executor:
Â  Â  Â  Â  future_gemini = executor.submit(lambda: fast_llm.invoke(query).content)
Â  Â  Â  Â  future_groq = executor.submit(query_groq, query, groq_api_key)
Â  Â  Â  Â  gemini_response = future_gemini.result()
Â  Â  Â  Â  groq_response = future_groq.result()

Â  Â  judge_prompt = f"""
Â  Â  You are an impartial AI evaluator. Compare two responses to a user's query and declare a winner.

Â  Â  ### User Query:
Â  Â  {query}
Â  Â  ### Response A (Gemini):
Â  Â  {gemini_response}
Â  Â  ### Response B (Groq):
Â  Â  {groq_response}

Â  Â  Instructions:
Â  Â  1. Begin with "Winner: Gemini" or "Winner: Groq".
Â  Â  2. Explain your reasoning clearly, comparing accuracy, clarity, and completeness.
Â  Â  """
Â  Â  judgment = judge_llm.invoke(judge_prompt).content
Â  Â Â 
Â  Â  match = re.search(r"winner\s*:\s*(gemini|groq)", judgment, re.IGNORECASE)
Â  Â  winner = match.group(1).capitalize() if match else "Evaluation"
Â  Â Â 
Â  Â  chosen_answer = gemini_response if winner == "Gemini" else groq_response
Â  Â Â 
Â  Â  final_output = f"## ðŸ† Judged Best Answer ({winner})\n"
Â  Â  final_output += f"{chosen_answer}\n\n"
Â  Â  final_output += f"### ðŸ§  Judge's Evaluation\n{judgment}\n\n---\n\n"
Â  Â  final_output += f"### Other Responses\n\n"
Â  Â  final_output += f"**ðŸ¤– Gemini's Full Response:**\n{gemini_response}\n\n"
Â  Â  final_output += f"**âš¡ Groq's Full Response:**\n{groq_response}"
Â  Â Â 
Â  Â  return final_output

# ===================================================================
# TOOL 2: IMAGE GENERATION TOOL
# ===================================================================
def image_generation_tool(prompt: str, google_api_key: str, pollinations_token: str) -> dict:
Â  Â  """Use this tool when the user asks to create, draw, or generate an image."""
Â  Â  logging.info(f"---TOOL: Generating Image for prompt: '{prompt}'---")
Â  Â  try:
Â  Â  Â  Â  enhancer_llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", google_api_key=google_api_key)
Â  Â  Â  Â  enhancer_prompt = f"Rewrite this short prompt into a detailed, vibrant, and artistic image generation description: {prompt}"
Â  Â  Â  Â  final_prompt = enhancer_llm.invoke(enhancer_prompt).content.strip()
Â  Â  Â  Â Â 
Â  Â  Â  Â  # --- FIX 1: URL Encode the prompt to handle special characters ---
Â  Â  Â  Â  encoded_prompt = quote_plus(final_prompt)
Â  Â  Â  Â Â 
Â  Â  Â  Â  url = f"https://image.pollinations.ai/prompt/{encoded_prompt}?token={pollinations_token}"
Â  Â  Â  Â Â 
Â  Â  Â  Â  # --- FIX 2: MODIFIED - Increased timeout to 180 seconds ---
Â  Â  Â  Â  response = requests.get(url, timeout=180)
Â  Â  Â  Â Â 
Â  Â  Â  Â  # --- FIX 3: Check if the request was successful before processing ---
Â  Â  Â  Â  response.raise_for_status()Â  # This will raise an error for bad status codes (4xx or 5xx)
Â  Â  Â  Â Â 
Â  Â  Â  Â  # If the above line passes, we know we have a valid response
Â  Â  Â  Â  img_bytes = response.content
Â  Â  Â  Â  img = Image.open(BytesIO(img_bytes))
Â  Â  Â  Â Â 
Â  Â  Â  Â  return {"image": img, "caption": f"Your prompt: '{prompt}'"}

Â  Â  except requests.exceptions.HTTPError as http_err:
Â  Â  Â  Â  logging.error(f"HTTP error occurred: {http_err} - Response: {response.text}")
Â  Â  Â  Â  return {"error": f"The image generation service returned an error: {http_err}"}
Â  Â Â 
Â  Â  # --- NEW: Catch ReadTimeout specifically to give a better error message ---
Â  Â  except requests.exceptions.ReadTimeout as timeout_err:
Â  Â  Â  Â  logging.error(f"Image generation timed out: {timeout_err}")
Â  Â  Â  Â  return {"error": "The image generation service timed out (took longer than 180s). It might be very busy. Please try again in a moment."}
Â  Â Â 
Â  Â  except Exception as e:
Â  Â  Â  Â  logging.error(f"An unexpected error occurred in image generation: {e}")
Â  Â  Â  Â  return {"error": f"Failed to generate image: {e}"}
# ===================================================================
# TOOL 3: FILE ANALYSIS TOOL (Streams output)
# ===================================================================

def file_analysis_tool(question: str, file_content_as_text: str, google_api_key: str):
Â  Â  """
Â  Â  Use this tool when the user has uploaded a file and is asking a question about it.
Â  Â  This tool is now empowered to use its own expertise to provide a comprehensive analysis.
Â  Â  """
Â  Â  print("---TOOL: Executing Empowered File Analysis---")
Â  Â  # Using a more powerful model for high-quality analysis and evaluation
Â  Â  streaming_llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", google_api_key=google_api_key, streaming=True)Â 
Â  Â Â 
Â  Â  # This new, "freer" prompt gives the agent permission to be a true expert.
Â  Â  prompt = f"""
Â  Â  **Your Persona:** You are a highly intelligent AI assistant and a multi-disciplinary expert. Your goal is to provide the most helpful and insightful analysis possible, combining the provided file content with your own vast knowledge.

Â  Â  **The Task:** A user has uploaded a file and asked a question. Use the file content as the primary source of truth and context, but you are encouraged to enrich your answer with your own expertise, especially when asked for an evaluation, opinion, or a subjective score.

Â  Â  **How to Behave Based on File Content:**
Â  Â  * **If the file appears to be CODE (e.g., Python, JavaScript, etc.):** Act as a senior software engineer. Analyze its structure, logic, efficiency, and style. If the user asks for a score or review, provide a thoughtful evaluation with justifications and suggestions for improvement.
Â  Â  * **If the file appears to be TEXT (e.g., an article, report, essay):** Act as a research analyst. Summarize key points, extract specific information, and answer the user's questions. You can add relevant context from your own knowledge if it enhances the answer (e.g., providing historical context for an article).
Â  Â  * **For all other file types:** Do your best to interpret the text content and provide a helpful, intelligent response to the user's question.

Â  Â  **User's Question:**
Â  Â  {question}

Â  Â  **Provided File Content:**
Â  Â  ---
Â  Â  {file_content_as_text[:40000]}Â 
Â  Â  ---

Â  Â  **Your Comprehensive Analysis:**
Â  Â  """
Â  Â  return streaming_llm.stream([HumanMessage(content=prompt)])

# ===================================================================
# THE AGENT: A "WORKSHOP MANAGER" THAT CHOOSES THE RIGHT TOOL
# ===================================================================

# ===================================================================
# NEW TOOL 3: WEB SEARCH & REAL-TIME DATA ANALYSIS
# ===================================================================
def web_search_tool(query: str, tavily_api_key: str, google_api_key: str) -> str:
Â  Â  """
Â  Â  Use this tool to get real-time information, answer questions about current events,
Â  Â  or for any query that requires up-to-date knowledge from the internet.
Â  Â  """
Â  Â  print("---TOOL: Executing Web Search and Analysis---")
Â  Â  try:
Â  Â  Â  Â  tavily = TavilyClient(api_key=tavily_api_key)
Â  Â  Â  Â  # Perform a search and get the most relevant results
Â  Â  Â  Â  search_results = tavily.search(query=query, search_depth="advanced", max_results=5)
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Extract the content from the search results
Â  Â  Â  Â  search_content = "\n".join([result["content"] for result in search_results["results"]])
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Use a powerful LLM to analyze the search results and answer the user's query
Â  Â  Â  Â  analyzer_llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", google_api_key=google_api_key)
Â  Â  Â  Â  analysis_prompt = f"""
Â  Â  Â  Â  You are an expert research analyst. You have been given a user's query and the results from a web search.
Â  Â  Â  Â  Your task is to provide a clear, concise, and comprehensive answer to the user's query based *only* on the provided search results.
Â  Â  Â  Â  Cite your sources using the information available in the search results if possible.

Â  Â  Â  Â  ### User Query:
Â  Â  Â  Â  {query}

Â  Â  Â  Â  ### Web Search Results:
Â  Â  Â  Â  ---
Â  Â  Â  Â  {search_content}
Â  Â  Â  Â  ---

Â  Â  Â  Â  Your Answer:
Â  Â  Â  Â  """
Â  Â  Â  Â  final_answer = analyzer_llm.invoke(analysis_prompt).content
Â  Â  Â  Â  return final_answer
Â  Â  Â  Â Â 
Â  Â  except Exception as e:
Â  Â  Â  Â  return f"âš ï¸ Web search failed: {e}"
Â  Â Â 
class AgentState(TypedDict):
Â  Â  query: str
Â  Â  route: strÂ  # Add this key to store the router's decision
Â  Â  final_response: Optional[any]
# These are wrapper functions for the nodes to handle passing state and API keys
def call_comparison_tool(state: AgentState, google_api_key: str, groq_api_key: str):
Â  Â  response = comparison_and_evaluation_tool(state['query'], google_api_key, groq_api_key)
Â  Â  return {"final_response": response}

def call_image_tool(state: AgentState, google_api_key: str, pollinations_token: str):
Â  Â  response = image_generation_tool(state['query'], google_api_key, pollinations_token)
Â  Â  return {"final_response": response}
def call_web_search_tool(state: AgentState, tavily_api_key: str, google_api_key: str):
Â  Â  response = web_search_tool(state['query'], tavily_api_key, google_api_key)
Â  Â  return {"final_response": response}

def router(state: AgentState, google_api_key: str):
Â  Â  """The brain of the agent. Decides which tool to use and updates the 'route' state key."""
Â  Â  print("---AGENT: Routing query---")
Â  Â  router_llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", google_api_key=google_api_key)
Â  Â  query = state['query']
Â  Â Â 
Â  Â  router_prompt = f"""
Â  Â  You are a master routing agent. Determine the user's primary intent and select the appropriate tool. You have two choices:
Â  Â  1.Â  `comparison_tool`: Use for complex questions, coding problems, analysis, or any text-based query needing a detailed, evaluated answer.
Â  Â  2.Â  `image_generation_tool`: Use ONLY if the user explicitly asks to create, draw, or generate an image.
Â  Â  3.Â  `web_search_tool`: Use this for any query that requires real-time, up-to-date information. This includes questions about current events, news, weather, recent scientific discoveries, or topics created after 2023.

Â  Â  User Query: "{query}"
Â  Â  Return ONLY the tool name (`comparison_tool` or `image_generation_tool` or `web_search_tool`).
Â  Â  """
Â  Â  response = router_llm.invoke(router_prompt).content.strip()
Â  Â Â 
Â  Â  if "web_search_tool" in response:
Â  Â  Â  Â  print("---AGENT: Decision -> Web Search Tool---")
Â  Â  Â  Â  return {"route": "web_search"}
Â  Â  elif "image_generation_tool" in response:
Â  Â  Â  Â  print("---AGENT: Decision -> Image Generation Tool---")
Â  Â  Â  Â  return {"route": "image_generator"}
Â  Â  else:
Â  Â  Â  Â  print("---AGENT: Decision -> Comparison & Evaluation Tool---")
Â  Â  Â  Â  return {"route": "comparison_chat"}
# --- Define the Agentic Graph ---
def build_agent(google_api_key: str, groq_api_key: str, pollinations_token: str , tavily_api_key: str ):
Â  Â  workflow = StateGraph(AgentState)

Â  Â  router_with_keys = partial(router, google_api_key=google_api_key)
Â  Â  comparison_node = partial(call_comparison_tool, google_api_key=google_api_key, groq_api_key=groq_api_key)
Â  Â  image_node = partial(call_image_tool, google_api_key=google_api_key, pollinations_token=pollinations_token)
Â  Â  web_search_node = partial(call_web_search_tool, tavily_api_key=tavily_api_key, google_api_key=google_api_key)

Â  Â  workflow.add_node("router", router_with_keys)
Â  Â  workflow.add_node("comparison_chat", comparison_node)
Â  Â  workflow.add_node("image_generator", image_node)
Â  Â  workflow.add_node("web_search", web_search_node)

Â  Â  workflow.set_entry_point("router")
Â  Â Â 
Â  Â  # The conditional edge function now simply reads the 'route' from the state
Â  Â  workflow.add_conditional_edges(
Â  Â  Â  Â  "router",
Â  Â  Â  Â  lambda state: state["route"],
Â  Â  Â  Â  {"comparison_chat": "comparison_chat", "image_generator": "image_generator" , "web_search": "web_search"}
Â  Â  )
Â  Â Â 
Â  Â  workflow.add_edge("comparison_chat", END)
Â  Â  workflow.add_edge("image_generator", END)
Â  Â  workflow.add_edge("web_search", END)
Â  Â  return workflow.compile() 
